## DarkLLM: Языковая Модель на PyTorch

DarkLLM - это исследовательский проект, в котором реализована простейшая языковая модель на основе архитектуры Transformer, используя библиотеку PyTorch. Модель обучается на текстовых данных, пытаясь предсказывать следующий символ в последовательности, тем самым генерируя текст.

### Принцип работы

Модель основана на архитектуре Transformer, которая использует механизм внимания для обработки последовательностей. В отличие от традиционных рекуррентных нейронных сетей, Transformer позволяет обрабатывать входные данные параллельно, что значительно ускоряет процесс обучения.

**Основные компоненты модели:**

* **Embedding слой:** Преобразует символы в векторное представление фиксированного размера.
* **Positional Encoding:** Добавляет информацию о позиции каждого символа в последовательности, так как Transformer не учитывает порядок символов по умолчанию.
* **Transformer Encoder-Decoder:** Основной блок модели, который обрабатывает входные данные с помощью механизма внимания и выполняет преобразования.
* **Linear слой:** Преобразует выход Transformer в вероятности для каждого символа в словаре.

### Обучение

Обучение модели происходит на текстовых данных, находящихся в директории `dataset`. Модель обучается предсказывать следующий символ в последовательности, используя функцию потерь CrossEntropyLoss и оптимизатор AdamW. 

Обучение модели проводилось на видеокарте NVIDIA RTX 3060 и заняло **11 часов**. Благодаря этому, модель способна формировать некоторые слова корректно, хотя для более сложных и связных текстов требуется дополнительное обучение и усовершенствование.

**Процесс обучения:**

1. **Чтение данных:** Загрузка текстовых данных из файлов в директории `dataset`.
2. **Создание словаря:** Построение словаря уникальных символов, обнаруженных в данных.
3. **Создание DataLoader:** Организация данных в батчи для эффективного обучения.
4. **Инициализация модели:** Создание экземпляра класса `CharTransformerModel` с заданными параметрами.
5. **Определение функции потерь и оптимизатора:** Использование CrossEntropyLoss и AdamW для оптимизации модели.
6. **Обучение модели:** Итеративное обновление весов модели с использованием данных из DataLoader.
7. **Сохранение чекпоинтов:** Периодическое сохранение состояния модели, чтобы можно было возобновить обучение в случае прерывания или использовать модель для генерации текста.

### Генерация текста (predict.py)

Скрипт `predict.py` позволяет генерировать текст с использованием обученной модели. 

**Процесс генерации:**

1. **Загрузка модели и словаря:** Загрузка весов модели и словаря из сохраненного чекпоинта.
2. **Получение начального текста:** Запрос у пользователя начальной строки, которая будет использована для генерации продолжения.
3. **Преобразование текста в числовое представление:** Преобразование символов начального текста в индексы с использованием словаря `stoi`.
4. **Генерация текста:** Итеративное предсказание следующего символа с использованием модели, добавление его к сгенерированному тексту и повторение процесса.
5. **Вывод сгенерированного текста:** Отображение сгенерированного текста пользователю по мере генерации.

### Структура проекта

```
DarkLLM/
├── analyze_dataset_file.py  
├── model.py                # Определение архитектуры модели
├── predict.py              # Скрипт для генерации текста
├── process_dataset.py      
├── ruscorpora_content.csv  
└── train.py                # Скрипт для обучения модели
```

### Планы на будущее

* **Реализация нормального токенизатора:** Вместо символьного подхода, планируется использовать токенизатор на уровне слов или подслов, что позволит модели лучше понимать структуру языка и генерировать более связные тексты. 
* **Расширение набора данных:** Использование более объемных и разнообразных наборов данных для обучения модели, что позволит улучшить ее качество и способность генерировать более сложные тексты.
* **Оптимизация модели:** Исследование различных архитектур и гиперпараметров для улучшения производительности и качества генерации.
* **Создание веб-интерфейса:** Разработка простого веб-интерфейса для удобного взаимодействия с моделью.
